{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiate between posts more commonly associated with confessions or relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a binary classifier to determine if a text of a post belongs to either the confessions or relationships class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import regex as re\n",
    "\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import  KNeighborsClassifier \n",
    "\n",
    "import warnings\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "# After the imports\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {'confessions' : 'https://www.reddit.com/r/confessions.json', \n",
    "        'relationships' : 'https://www.reddit.com/r/relationships.json'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap data using the PushShiftAPI to extract more than 1000 posts per subreddit to overcome Reddit's imposed limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "api = PushshiftAPI()\n",
    "confessions = pd.DataFrame(list(api.search_submissions(subreddit='confessions',\n",
    "                                         filter=['author','title','subreddit','selftext'],\n",
    "                                         limit=3000)))\n",
    "relationships = pd.DataFrame(list(api.search_submissions(subreddit='relationships',\n",
    "                                         filter=['author','title','subreddit','selftext'],\n",
    "                                         limit=3000)))\n",
    "\n",
    "# store the scrapped data.\n",
    "confessions.to_csv('./data/confessions.csv')\n",
    "relationships.to_csv('./data/relationships.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "## Scrapper using conventional methods\n",
    "# def reddit_scrapper(key,url,n_iterations=10):\n",
    "    \n",
    "#     #load_previous_file\n",
    "#     prev_posts = pd.read_csv('./data/' + str(key) + '.csv')\n",
    "#     print(\"Number of records loaded : {}\".format(prev_posts.shape[0]))\n",
    "    \n",
    "#     posts = []\n",
    "#     after = None\n",
    "\n",
    "#     for a in range(n_iterations):\n",
    "#         if after == None:\n",
    "#             current_url = url + '?limit=100'\n",
    "#         else:\n",
    "#             current_url = url + '?after=' + after + '&limit=100'\n",
    "#         print(current_url)\n",
    "#         res = requests.get(current_url, headers={'User-agent': 'Falcon 2.0'})\n",
    "\n",
    "#         if res.status_code != 200:\n",
    "#             print('Status error', res.status_code)\n",
    "#             break\n",
    "\n",
    "#         current_dict = res.json()\n",
    "#         current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "#         posts.extend(current_posts)\n",
    "#         after = current_dict['data']['after']\n",
    "\n",
    "#         # generate a random sleep duration to look more 'natural'\n",
    "#         sleep_duration = random.randint(2,6)\n",
    "        \n",
    "#         time.sleep(sleep_duration)\n",
    "    \n",
    "#     #add_to_existing\n",
    "#     posts = pd.DataFrame(posts)\n",
    "#     posts_df = posts.append(prev_posts,ignore_index=True)\n",
    "#     #remove duplicates\n",
    "#     #posts_df.drop_duplicates(inplace=True)\n",
    "#     print(\"Number of records stored : {}\".format(posts_df.shape[0]))\n",
    "#     posts_df.to_csv('./data/' + str(key) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships = pd.read_csv('./data/relationships.csv')\n",
    "df_confessions = pd.read_csv('./data/confessions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `filter_columns` function that filters out the title, self text and subreddit name (our target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(df):\n",
    "    columns_to_retain = ['title','selftext','subreddit','author']\n",
    "    return df[columns_to_retain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean = filter_columns(df_relationships)\n",
    "df_conf_clean = filter_columns(df_confessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        3000\n",
       "selftext     2985\n",
       "subreddit    3000\n",
       "author       3000\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "title        3000\n",
       "selftext     2507\n",
       "subreddit    3000\n",
       "author       3000\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_relationships_clean.count())\n",
    "display(df_conf_clean.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the classes are imbalanced. For our classification dataset, we will aim to have a 1:1 class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I (20F) don't know if I want to see my LDR(19M...</td>\n",
       "      <td>I don't know if I want to see my LDR anymore, ...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>another4ccount12345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does this mean?</td>\n",
       "      <td>Ex got very pissed off and mad when I said tha...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>horse126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Friend (22M) is dating my (22M) sister (20F). ...</td>\n",
       "      <td>My friend is dating my sister. At first I was ...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>turndownforcat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oof</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>relationships</td>\n",
       "      <td>kelsonyt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I unfairly compare everyone to my ex and I wis...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>relationships</td>\n",
       "      <td>babelfiish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  I (20F) don't know if I want to see my LDR(19M...   \n",
       "1                               What does this mean?   \n",
       "2  Friend (22M) is dating my (22M) sister (20F). ...   \n",
       "3                                                Oof   \n",
       "4  I unfairly compare everyone to my ex and I wis...   \n",
       "\n",
       "                                            selftext      subreddit  \\\n",
       "0  I don't know if I want to see my LDR anymore, ...  relationships   \n",
       "1  Ex got very pissed off and mad when I said tha...  relationships   \n",
       "2  My friend is dating my sister. At first I was ...  relationships   \n",
       "3                                          [removed]  relationships   \n",
       "4                                          [removed]  relationships   \n",
       "\n",
       "                author  \n",
       "0  another4ccount12345  \n",
       "1             horse126  \n",
       "2       turndownforcat  \n",
       "3             kelsonyt  \n",
       "4           babelfiish  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relationships_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I lost my virginity in a 3 way</td>\n",
       "      <td>Im a 19 year old male in a rural town. This ha...</td>\n",
       "      <td>confessions</td>\n",
       "      <td>jdallis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I blame myself</td>\n",
       "      <td>I’m a woman in my early 30s. I’m a lesbian and...</td>\n",
       "      <td>confessions</td>\n",
       "      <td>heynotyouagain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm just realizing that I was assaulted, 7 yea...</td>\n",
       "      <td>I broke up with my ex-boyfriend because he was...</td>\n",
       "      <td>confessions</td>\n",
       "      <td>will0w27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I mistakenly sent an intimate video of my GF a...</td>\n",
       "      <td>This is a long, long post. Full of an excessiv...</td>\n",
       "      <td>confessions</td>\n",
       "      <td>IamAFUkkingIdiot3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m 16, and I’m going to court tomorrow.</td>\n",
       "      <td>Long story short, I got caught by the cops smo...</td>\n",
       "      <td>confessions</td>\n",
       "      <td>holygift462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                     I lost my virginity in a 3 way   \n",
       "1                                     I blame myself   \n",
       "2  I'm just realizing that I was assaulted, 7 yea...   \n",
       "3  I mistakenly sent an intimate video of my GF a...   \n",
       "4           I’m 16, and I’m going to court tomorrow.   \n",
       "\n",
       "                                            selftext    subreddit  \\\n",
       "0  Im a 19 year old male in a rural town. This ha...  confessions   \n",
       "1  I’m a woman in my early 30s. I’m a lesbian and...  confessions   \n",
       "2  I broke up with my ex-boyfriend because he was...  confessions   \n",
       "3  This is a long, long post. Full of an excessiv...  confessions   \n",
       "4  Long story short, I got caught by the cops smo...  confessions   \n",
       "\n",
       "              author  \n",
       "0            jdallis  \n",
       "1     heynotyouagain  \n",
       "2           will0w27  \n",
       "3  IamAFUkkingIdiot3  \n",
       "4        holygift462  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conf_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to this, we may wish to remove posts that have 'Moderator' as an author to train our model on more 'authentic' posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean.loc[:,'author'] = df_relationships_clean.author.map(lambda x : x.lower())\n",
    "df_conf_clean.loc[:,'author'] = df_conf_clean.author.map(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean = df_relationships_clean[~df_relationships_clean.author.str.contains('moderator')]\n",
    "df_conf_clean = df_conf_clean[~df_conf_clean.author.str.contains('moderator')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title         0\n",
       "selftext     15\n",
       "subreddit     0\n",
       "author        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relationships_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "selftext     493\n",
       "subreddit      0\n",
       "author         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conf_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also observe empty selftext in both subreddits. we shall drop rows with empty selftext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean = df_relationships_clean.dropna(axis=0)\n",
    "df_conf_clean = df_conf_clean.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure only posts with selftext more than 10words are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean ['selftext_len'] = df_relationships_clean .selftext.map(lambda x: len(x.split()))\n",
    "df_relationships_clean  = df_relationships_clean [df_relationships_clean .selftext_len > 10]\n",
    "df_conf_clean['selftext_len'] = df_conf_clean.selftext.map(lambda x: len(x.split()))\n",
    "df_conf_clean = df_conf_clean[df_conf_clean.selftext_len > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean.drop_duplicates(inplace=True)\n",
    "df_conf_clean.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           1801\n",
       "selftext        1801\n",
       "subreddit       1801\n",
       "author          1801\n",
       "selftext_len    1801\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "title           2286\n",
       "selftext        2286\n",
       "subreddit       2286\n",
       "author          2286\n",
       "selftext_len    2286\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_relationships_clean.count())\n",
    "display(df_conf_clean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deleted] Counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    1801\n",
       "Name: title, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False    2286\n",
       "Name: title, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[removed] Counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    1801\n",
       "Name: title, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False    2286\n",
       "Name: title, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check posts with [deleted] or [removed]\n",
    "print(\"[deleted] Counts:\")\n",
    "display((df_relationships_clean.title == '[deleted]').value_counts())\n",
    "display((df_conf_clean.title == '[deleted]').value_counts())\n",
    "print(\"[removed] Counts:\")\n",
    "display((df_relationships_clean.title == '[removed]').value_counts())\n",
    "display((df_conf_clean.title == '[removed]').value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then randomly select 1800 of each class since quite a significant number were from a moderator-author as well as empty text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_relationships_clean = df_relationships_clean.sample(n=1800,random_state=666)\n",
    "subset_conf_clean = df_conf_clean.sample(n=1800,random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confessions      1800\n",
       "relationships    1800\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine both subsets into a DF\n",
    "df_pre = subset_relationships_clean.append(subset_conf_clean,ignore_index=True)\n",
    "df_pre.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext_len</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I (23F) can't stand my toxic boyfriend (24M) w...</td>\n",
       "      <td>I've been dating my boyfriend for about 3.5 ye...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>faultless_to_a_fault</td>\n",
       "      <td>421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I [24m] knocked up my girlfriend [22f] of nine...</td>\n",
       "      <td>As the title says, I've been with my girlfrien...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>substantial_program</td>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My BFF (17/F) keeps lying and breaks my (15/F)...</td>\n",
       "      <td>This is a teens argument so i apologise\\n in a...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>stalinesexslave2</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I (M,40) absolutely adore my partner (F,39) bu...</td>\n",
       "      <td>I’m a m of 40 and my partner is 39. We have be...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>iam_mrgee</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My Boyfriend (25m) Called me (28f) Fat Ass.</td>\n",
       "      <td>I dont want to make this a long post so I'm go...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>smellslikecheerios</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  I (23F) can't stand my toxic boyfriend (24M) w...   \n",
       "1  I [24m] knocked up my girlfriend [22f] of nine...   \n",
       "2  My BFF (17/F) keeps lying and breaks my (15/F)...   \n",
       "3  I (M,40) absolutely adore my partner (F,39) bu...   \n",
       "4        My Boyfriend (25m) Called me (28f) Fat Ass.   \n",
       "\n",
       "                                            selftext      subreddit  \\\n",
       "0  I've been dating my boyfriend for about 3.5 ye...  relationships   \n",
       "1  As the title says, I've been with my girlfrien...  relationships   \n",
       "2  This is a teens argument so i apologise\\n in a...  relationships   \n",
       "3  I’m a m of 40 and my partner is 39. We have be...  relationships   \n",
       "4  I dont want to make this a long post so I'm go...  relationships   \n",
       "\n",
       "                 author  selftext_len label  \n",
       "0  faultless_to_a_fault           421     0  \n",
       "1   substantial_program           305     0  \n",
       "2      stalinesexslave2           484     0  \n",
       "3             iam_mrgee           367     0  \n",
       "4    smellslikecheerios           310     0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create target class columns 0 = relationships, 1 = confessions \n",
    "\n",
    "df_pre['label'] = df_pre.subreddit.map({'relationships':'0','confessions':'1'}) \n",
    "df_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure formatting of text by:\n",
    "- Converting all to lower cases\n",
    "- removing groups of words in parantheses\n",
    "- remove line breaks\n",
    "- removing special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the stop words to a set.\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    #01 convert titles, selftext into lowercase\n",
    "    lower_text = text.lower()\n",
    "    #02 remove brackets and parenthesis from the title and selftext.\n",
    "    no_br_paret_text = re.sub(r'\\(.+?\\)|\\[.+?\\]',' ',str(lower_text))\n",
    "    #03 remove special characters\n",
    "    removed_special = re.sub(r'[^0-9a-zA-Z ]+',' ',str(no_br_paret_text))\n",
    "    #04 remove xamp200b\n",
    "    remove_xamp200b = re.sub(r'ampx200b',' ',str(removed_special))\n",
    "    #05 remove digits\n",
    "    result = re.sub(r'\\d+', '', remove_xamp200b).split()\n",
    "    #05 split into individual words\n",
    "    meaningful_words = [w for w in result if not w in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext_len</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stand toxic boyfriend game together stop ruini...</td>\n",
       "      <td>dating boyfriend years rough patches pretty go...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>faultless_to_a_fault</td>\n",
       "      <td>421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knocked girlfriend nine months terrible idea g...</td>\n",
       "      <td>title says girlfriend nine months going really...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>substantial_program</td>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bff keeps lying breaks heart talk help</td>\n",
       "      <td>teens argument apologise advance long boring e...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>stalinesexslave2</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>absolutely adore partner feel like interested ...</td>\n",
       "      <td>partner together years two children youngest y...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>iam_mrgee</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boyfriend called fat ass</td>\n",
       "      <td>dont want make long post going try make easy r...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>smellslikecheerios</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  stand toxic boyfriend game together stop ruini...   \n",
       "1  knocked girlfriend nine months terrible idea g...   \n",
       "2             bff keeps lying breaks heart talk help   \n",
       "3  absolutely adore partner feel like interested ...   \n",
       "4                           boyfriend called fat ass   \n",
       "\n",
       "                                            selftext      subreddit  \\\n",
       "0  dating boyfriend years rough patches pretty go...  relationships   \n",
       "1  title says girlfriend nine months going really...  relationships   \n",
       "2  teens argument apologise advance long boring e...  relationships   \n",
       "3  partner together years two children youngest y...  relationships   \n",
       "4  dont want make long post going try make easy r...  relationships   \n",
       "\n",
       "                 author  selftext_len label  \n",
       "0  faultless_to_a_fault           421     0  \n",
       "1   substantial_program           305     0  \n",
       "2      stalinesexslave2           484     0  \n",
       "3             iam_mrgee           367     0  \n",
       "4    smellslikecheerios           310     0  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title','selftext']] = df_pre[['title','selftext']].applymap(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've been dating my boyfriend for about 3.5 ye...</td>\n",
       "      <td>dating boyfriend years rough patches pretty go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As the title says, I've been with my girlfrien...</td>\n",
       "      <td>title says girlfriend nine months going really...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a teens argument so i apologise\\n in a...</td>\n",
       "      <td>teens argument apologise advance long boring e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I’m a m of 40 and my partner is 39. We have be...</td>\n",
       "      <td>partner together years two children youngest y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I dont want to make this a long post so I'm go...</td>\n",
       "      <td>dont want make long post going try make easy r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 pre  \\\n",
       "0  I've been dating my boyfriend for about 3.5 ye...   \n",
       "1  As the title says, I've been with my girlfrien...   \n",
       "2  This is a teens argument so i apologise\\n in a...   \n",
       "3  I’m a m of 40 and my partner is 39. We have be...   \n",
       "4  I dont want to make this a long post so I'm go...   \n",
       "\n",
       "                                                post  \n",
       "0  dating boyfriend years rough patches pretty go...  \n",
       "1  title says girlfriend nine months going really...  \n",
       "2  teens argument apologise advance long boring e...  \n",
       "3  partner together years two children youngest y...  \n",
       "4  dont want make long post going try make easy r...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=zip(df_pre['selftext'],df['selftext']),columns=['pre','post']).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split title and self text into two classifiers where the output of title_classifier and self_text classifier would provide indication of subreddit belonging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split titles, and self text into seperate df\n",
    "\n",
    "df_title = df[['title','label']]\n",
    "df_selftext = df[['selftext','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_words(sparse_counts, columns):\n",
    "    # X_all is a sparse matrix, so sum() returns a 'matrix' datatype ...\n",
    "    #   which we then convert into a 1-D ndarray for sorting\n",
    "    word_counts = np.asarray(sparse_counts.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    # argsort() returns smallest first, so we reverse the result\n",
    "    largest_count_indices = word_counts.argsort()[::-1]\n",
    "\n",
    "    # pretty-print the results! Remember to always ask whether they make sense ...\n",
    "    freq_words = pd.Series(word_counts[largest_count_indices], \n",
    "                           index=columns[largest_count_indices])\n",
    "\n",
    "    return freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the CountVectorizer to count words for us for each class\n",
    "\n",
    "# create mask\n",
    "\n",
    "X_1 = df_selftext[df_selftext['label'] == '1']\n",
    "X_0 = df_selftext[df_selftext['label'] == '0']\n",
    "\n",
    "cvt      =  CountVectorizer(ngram_range=(1,1),stop_words='english')\n",
    "X_1_all    =  cvt.fit_transform(X_1['selftext'])\n",
    "X_0_all    =  cvt.fit_transform(X_0['selftext'])\n",
    "columns_1  =  np.array(cvt.get_feature_names())          # ndarray (for indexing below)\n",
    "columns_0  =  np.array(cvt.get_feature_names())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confessions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "landed           1870\n",
       "jaunt            1272\n",
       "fallacies        1035\n",
       "tactics          1025\n",
       "unreal            951\n",
       "psychological     903\n",
       "owe               840\n",
       "genre             699\n",
       "laconically       680\n",
       "sushi             659\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationships:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "like            4112\n",
       "time            3048\n",
       "know            2949\n",
       "want            2740\n",
       "feel            2528\n",
       "really          2485\n",
       "relationship    2314\n",
       "said            2122\n",
       "things          1960\n",
       "told            1888\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_words_1 = get_freq_words(X_1_all, columns_1)\n",
    "freq_words_0 = get_freq_words(X_0_all, columns_0)\n",
    "print('Confessions:')\n",
    "display(freq_words_1[:10])\n",
    "print('Relationships:')\n",
    "display(freq_words_0[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split selftext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df_selftext['selftext']\n",
    "y_text = df_selftext['label']\n",
    "\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text,y_text,stratify=y_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Naive Bayers model - MultinomialNB as there are multiple nominal features in the form of the various tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   11.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.73      0.96      0.83       450\n",
      "     class 1       0.94      0.65      0.77       450\n",
      "\n",
      "    accuracy                           0.80       900\n",
      "   macro avg       0.84      0.80      0.80       900\n",
      "weighted avg       0.84      0.80      0.80       900\n",
      "\n",
      "Cross val score for mnb classifier using cvec vectorizer is [0.79444444 0.78703704 0.80740741 0.83518519 0.78888889]\n",
      "Accuracy score for mnb classifier using cvec vectorizer is 0.8044444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   10.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.62      1.00      0.77       450\n",
      "     class 1       0.99      0.39      0.56       450\n",
      "\n",
      "    accuracy                           0.69       900\n",
      "   macro avg       0.81      0.69      0.66       900\n",
      "weighted avg       0.81      0.69      0.66       900\n",
      "\n",
      "Cross val score for mnb classifier using tfvec vectorizer is [0.7        0.66111111 0.67407407 0.71481481 0.67592593]\n",
      "Accuracy score for mnb classifier using tfvec vectorizer is 0.6944444444444444\n"
     ]
    }
   ],
   "source": [
    "classifiers = [('mnb', MultinomialNB())]\n",
    "vectorizers = [('cvec', CountVectorizer(stop_words='english',tokenizer=LemmaTokenizer())),\n",
    "              ('tfvec', TfidfVectorizer(stop_words='english',tokenizer=LemmaTokenizer()))]\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    bayes_pipe = Pipeline([\n",
    "            (vectorizer),\n",
    "            (classifiers[0])\n",
    "        ])\n",
    "    scores = cross_val_score(bayes_pipe, X_text_train, y_text_train,cv=5,verbose=1)\n",
    "    b = bayes_pipe.fit(X_text_train, y_text_train)\n",
    "    y_pred = bayes_pipe.predict(X_text_test)\n",
    "    print(classification_report(y_text_test, y_pred, target_names=['class 0','class 1']))\n",
    "    print('Cross val score for {} classifier using {} vectorizer is {}'.format(classifiers[0][0],vectorizer[0],scores))\n",
    "    print('Accuracy score for {} classifier using {} vectorizer is {}'.format(classifiers[0][0],vectorizer[0],bayes_pipe.score(X_text_test, y_text_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the recall scores for multonomial NB with countvectorizer seems to provide higher recall when compared to the tfidf vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words='english',tokenizer=LemmaTokenizer())),\n",
    "    ('lr', LogisticRegression(solver='saga',max_iter=300))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform GridSearchCV to get best hyperparameters for our vectorizer and logistic regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 25.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 40.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed: 55.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8581481481481481\n"
     ]
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2500, 3000, 3500],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__penalty' : ['elasticnet'],\n",
    "    'lr__C' : np.arange(0.1,1,0.1),\n",
    "    'lr__l1_ratio' : np.arange(0,1.1,0.2)\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=5,verbose=1,n_jobs=-1)\n",
    "gs.fit(X_text_train, y_text_train)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 3500,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'lr__C': 0.1,\n",
       " 'lr__l1_ratio': 0.0,\n",
       " 'lr__penalty': 'elasticnet'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_.score(X_text_test,y_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try model on title\n",
    "optimal_pipe = Pipeline([\n",
    "            ('cvec', CountVectorizer(tokenizer=LemmaTokenizer(),max_features=3500,ngram_range=(1,1))),\n",
    "            ('lr', LogisticRegression(solver='saga',max_iter=300,C=0.1,l1_ratio=0.0,penalty='elasticnet'))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_title = df_title['title']\n",
    "y_title = df_title['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cvec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=3500, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<__main__.LemmaTokenizer object at 0x7f2b10030780>,\n",
       "                                 vocabulary=None)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=0.1, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=0.0, max_iter=300,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='elasticnet', random_state=None,\n",
       "                                    solver='saga', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_pipe.fit(X_text_train, y_text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try the model on our `title` dataset to obtain the accuracy of the model to classify the subreddit from titles alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.77      0.93      0.84       450\n",
      "     class 1       0.91      0.73      0.81       450\n",
      "\n",
      "    accuracy                           0.83       900\n",
      "   macro avg       0.84      0.83      0.82       900\n",
      "weighted avg       0.84      0.83      0.82       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_logr_pred = optimal_pipe.predict(X_text_test)\n",
    "print(classification_report(y_text_test, y_logr_pred, target_names=['class 0','class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we explore the use tfidfvectorizer instead of countvectorizer to account for document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 972 out of 972 | elapsed: 12.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9077777777777778\n"
     ]
    }
   ],
   "source": [
    "tfidf_pipe = Pipeline([\n",
    "    ('tfvec', TfidfVectorizer(stop_words='english',tokenizer=LemmaTokenizer())),\n",
    "    ('lr', LogisticRegression(solver='saga',max_iter=300))\n",
    "])\n",
    "\n",
    "tfidf_params = {\n",
    "    'tfvec__max_features': [2500, 3000, 3500],\n",
    "    'tfvec__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__penalty' : ['elasticnet'],\n",
    "    'lr__C' : np.arange(0.1,1,0.1),\n",
    "    'lr__l1_ratio' : np.arange(0,1.1,0.2)\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(tfidf_pipe, param_grid=tfidf_params, cv=3,verbose=1,n_jobs=-1)\n",
    "gs.fit(X_text_train, y_text_train)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that tfidf vectorizer performs best with the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 0.9,\n",
       " 'lr__l1_ratio': 1.0,\n",
       " 'lr__penalty': 'elasticnet',\n",
       " 'tfvec__max_features': 3500,\n",
       " 'tfvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Report (results based on test data) \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.91      0.88      0.90       450\n",
      "     class 1       0.89      0.92      0.90       450\n",
      "\n",
      "    accuracy                           0.90       900\n",
      "   macro avg       0.90      0.90      0.90       900\n",
      "weighted avg       0.90      0.90      0.90       900\n",
      "\n",
      "Titles (all titles) Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.16      0.28      1800\n",
      "     class 1       0.54      0.99      0.70      1800\n",
      "\n",
      "    accuracy                           0.57      3600\n",
      "   macro avg       0.73      0.57      0.49      3600\n",
      "weighted avg       0.73      0.57      0.49      3600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test model against test text data and rest of titles\n",
    "y_text_tfidf_pred = gs.best_estimator_.predict(X_text_test)\n",
    "y_title_tfidf_pred = gs.best_estimator_.predict(X_title)\n",
    "print(\"Text Report (results based on test data) \\n\" + \n",
    "      classification_report(y_text_test, y_text_tfidf_pred, target_names=['class 0','class 1']))\n",
    "print(\"Titles (all titles) Report \\n\" + \n",
    "      classification_report(y_title, y_title_tfidf_pred, target_names=['class 0','class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the optimised model with tfidf vectorizer performs remarkably well with high precision and recall, when used with the titles dataset, we can see that that it is somewhat overfit, unable to classify the titles correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bi woman personally call bi sake like women mu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16 mom dad 4 kids divorce little six years ago...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13 months sober 2 shots snuck wtf</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>told months ago happened many years ago long e...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probably one hardest things ever type right ty...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text actual predicted\n",
       "0  bi woman personally call bi sake like women mu...      1         1\n",
       "1  16 mom dad 4 kids divorce little six years ago...      0         0\n",
       "2                  13 months sober 2 shots snuck wtf      1         1\n",
       "3  told months ago happened many years ago long e...      1         1\n",
       "4  probably one hardest things ever type right ty...      1         0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at sample predictions\n",
    "\n",
    "pd.DataFrame(data=zip(X_text_test,y_text_test,y_text_tfidf_pred),columns=['text','actual','predicted']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline([\n",
    "    ('tfvec', TfidfVectorizer(stop_words='english',tokenizer=LemmaTokenizer())),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "knn_params = {\n",
    "    'tfvec__max_features': [2500, 3000, 3500],\n",
    "    'tfvec__ngram_range': [(1,1), (1,2)],\n",
    "    'knn__n_neighbors' : np.arange(5,100,10),\n",
    "    'knn__weights' : ['uniform','distance'],\n",
    "    'knn__leaf_size': np.arange(10,30,10) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  9.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfvec',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_word...\n",
       "                                                             weights='uniform'))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'knn__leaf_size': array([10, 20]),\n",
       "                         'knn__n_neighbors': array([ 5, 15, 25, 35, 45, 55, 65, 75, 85, 95]),\n",
       "                         'knn__weights': ['uniform', 'distance'],\n",
       "                         'tfvec__max_features': [2500, 3000, 3500],\n",
       "                         'tfvec__ngram_range': [(1, 1), (1, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gs = GridSearchCV(knn_pipe, param_grid=knn_params, cv=3,verbose=1,n_jobs=-1)\n",
    "knn_gs.fit(X_text_train, y_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn__leaf_size': 10,\n",
       " 'knn__n_neighbors': 15,\n",
       " 'knn__weights': 'distance',\n",
       " 'tfvec__max_features': 2500,\n",
       " 'tfvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Report (results based on test data) \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.82      0.76      0.79       450\n",
      "     class 1       0.78      0.84      0.81       450\n",
      "\n",
      "    accuracy                           0.80       900\n",
      "   macro avg       0.80      0.80      0.80       900\n",
      "weighted avg       0.80      0.80      0.80       900\n",
      "\n",
      "Titles (all titles) Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.82      0.36      0.51      1800\n",
      "     class 1       0.59      0.92      0.72      1800\n",
      "\n",
      "    accuracy                           0.64      3600\n",
      "   macro avg       0.71      0.64      0.61      3600\n",
      "weighted avg       0.71      0.64      0.61      3600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test model against test text data and rest of titles\n",
    "y_text_knn_pred = knn_gs.best_estimator_.predict(X_text_test)\n",
    "y_title_knn_pred = knn_gs.best_estimator_.predict(X_title)\n",
    "print(\"Text Report (results based on test data) \\n\" + \n",
    "      classification_report(y_text_test, y_text_knn_pred, target_names=['class 0','class 1']))\n",
    "print(\"Titles (all titles) Report \\n\" + \n",
    "      classification_report(y_title, y_title_knn_pred, target_names=['class 0','class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
