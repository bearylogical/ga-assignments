{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiate between posts more commonly associated with confessions or relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a binary classifier to determine if a text of a post belongs to either the confessions or relationships class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import regex as re\n",
    "\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import  KNeighborsClassifier \n",
    "\n",
    "import warnings\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "# After the imports\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {'confessions' : 'https://www.reddit.com/r/confessions.json', \n",
    "        'relationships' : 'https://www.reddit.com/r/relationships.json'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap data using the PushShiftAPI to extract more than 1000 posts per subreddit to overcome Reddit's imposed limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "api = PushshiftAPI()\n",
    "confessions = pd.DataFrame(list(api.search_submissions(subreddit='confessions',\n",
    "                                         filter=['author','title','subreddit','selftext'],\n",
    "                                         limit=3000)))\n",
    "relationships = pd.DataFrame(list(api.search_submissions(subreddit='relationships',\n",
    "                                         filter=['author','title','subreddit','selftext'],\n",
    "                                         limit=3000)))\n",
    "\n",
    "# store the scrapped data.\n",
    "confessions.to_csv('./data/confessions.csv')\n",
    "relationships.to_csv('./data/relationships.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "## Scrapper using conventional methods\n",
    "# def reddit_scrapper(key,url,n_iterations=10):\n",
    "    \n",
    "#     #load_previous_file\n",
    "#     prev_posts = pd.read_csv('./data/' + str(key) + '.csv')\n",
    "#     print(\"Number of records loaded : {}\".format(prev_posts.shape[0]))\n",
    "    \n",
    "#     posts = []\n",
    "#     after = None\n",
    "\n",
    "#     for a in range(n_iterations):\n",
    "#         if after == None:\n",
    "#             current_url = url + '?limit=100'\n",
    "#         else:\n",
    "#             current_url = url + '?after=' + after + '&limit=100'\n",
    "#         print(current_url)\n",
    "#         res = requests.get(current_url, headers={'User-agent': 'Falcon 2.0'})\n",
    "\n",
    "#         if res.status_code != 200:\n",
    "#             print('Status error', res.status_code)\n",
    "#             break\n",
    "\n",
    "#         current_dict = res.json()\n",
    "#         current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "#         posts.extend(current_posts)\n",
    "#         after = current_dict['data']['after']\n",
    "\n",
    "#         # generate a random sleep duration to look more 'natural'\n",
    "#         sleep_duration = random.randint(2,6)\n",
    "        \n",
    "#         time.sleep(sleep_duration)\n",
    "    \n",
    "#     #add_to_existing\n",
    "#     posts = pd.DataFrame(posts)\n",
    "#     posts_df = posts.append(prev_posts,ignore_index=True)\n",
    "#     #remove duplicates\n",
    "#     #posts_df.drop_duplicates(inplace=True)\n",
    "#     print(\"Number of records stored : {}\".format(posts_df.shape[0]))\n",
    "#     posts_df.to_csv('./data/' + str(key) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships = pd.read_csv('./data/relationships.csv')\n",
    "df_confessions = pd.read_csv('./data/confessions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `filter_columns` function that filters out the title, self text and subreddit name (our target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(df):\n",
    "    columns_to_retain = ['title','selftext','subreddit','author']\n",
    "    return df[columns_to_retain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean = filter_columns(df_relationships)\n",
    "df_conf_clean = filter_columns(df_confessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        3000\n",
       "selftext     2985\n",
       "subreddit    3000\n",
       "author       3000\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "title        3000\n",
       "selftext     2507\n",
       "subreddit    3000\n",
       "author       3000\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_relationships_clean.count())\n",
    "display(df_conf_clean.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the classes are imbalanced. For our classification dataset, we will aim to have a 1:1 class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I (20F) don't know if I want to see my LDR(19M...</td>\n",
       "      <td>I don't know if I want to see my LDR anymore, ...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>another4ccount12345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does this mean?</td>\n",
       "      <td>Ex got very pissed off and mad when I said tha...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>horse126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Friend (22M) is dating my (22M) sister (20F). ...</td>\n",
       "      <td>My friend is dating my sister. At first I was ...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>turndownforcat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oof</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>relationships</td>\n",
       "      <td>kelsonyt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I unfairly compare everyone to my ex and I wis...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>relationships</td>\n",
       "      <td>babelfiish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  I (20F) don't know if I want to see my LDR(19M...   \n",
       "1                               What does this mean?   \n",
       "2  Friend (22M) is dating my (22M) sister (20F). ...   \n",
       "3                                                Oof   \n",
       "4  I unfairly compare everyone to my ex and I wis...   \n",
       "\n",
       "                                            selftext      subreddit  \\\n",
       "0  I don't know if I want to see my LDR anymore, ...  relationships   \n",
       "1  Ex got very pissed off and mad when I said tha...  relationships   \n",
       "2  My friend is dating my sister. At first I was ...  relationships   \n",
       "3                                          [removed]  relationships   \n",
       "4                                          [removed]  relationships   \n",
       "\n",
       "                author  \n",
       "0  another4ccount12345  \n",
       "1             horse126  \n",
       "2       turndownforcat  \n",
       "3             kelsonyt  \n",
       "4           babelfiish  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relationships_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I lost my virginity in a 3 way</td>\n",
       "      <td>Im a 19 year old male in a rural town. This ha...</td>\n",
       "      <td>confessions</td>\n",
       "      <td>jdallis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I blame myself</td>\n",
       "      <td>Iâm a woman in my early 30s. Iâm a lesbian and...</td>\n",
       "      <td>confessions</td>\n",
       "      <td>heynotyouagain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm just realizing that I was assaulted, 7 yea...</td>\n",
       "      <td>I broke up with my ex-boyfriend because he was...</td>\n",
       "      <td>confessions</td>\n",
       "      <td>will0w27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I mistakenly sent an intimate video of my GF a...</td>\n",
       "      <td>This is a long, long post. Full of an excessiv...</td>\n",
       "      <td>confessions</td>\n",
       "      <td>IamAFUkkingIdiot3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iâm 16, and Iâm going to court tomorrow.</td>\n",
       "      <td>Long story short, I got caught by the cops smo...</td>\n",
       "      <td>confessions</td>\n",
       "      <td>holygift462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                     I lost my virginity in a 3 way   \n",
       "1                                     I blame myself   \n",
       "2  I'm just realizing that I was assaulted, 7 yea...   \n",
       "3  I mistakenly sent an intimate video of my GF a...   \n",
       "4           Iâm 16, and Iâm going to court tomorrow.   \n",
       "\n",
       "                                            selftext    subreddit  \\\n",
       "0  Im a 19 year old male in a rural town. This ha...  confessions   \n",
       "1  Iâm a woman in my early 30s. Iâm a lesbian and...  confessions   \n",
       "2  I broke up with my ex-boyfriend because he was...  confessions   \n",
       "3  This is a long, long post. Full of an excessiv...  confessions   \n",
       "4  Long story short, I got caught by the cops smo...  confessions   \n",
       "\n",
       "              author  \n",
       "0            jdallis  \n",
       "1     heynotyouagain  \n",
       "2           will0w27  \n",
       "3  IamAFUkkingIdiot3  \n",
       "4        holygift462  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conf_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to this, we may wish to remove posts that have 'Moderator' as an author to train our model on more 'authentic' posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean.loc[:,'author'] = df_relationships_clean.author.map(lambda x : x.lower())\n",
    "df_conf_clean.loc[:,'author'] = df_conf_clean.author.map(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean = df_relationships_clean[~df_relationships_clean.author.str.contains('moderator')]\n",
    "df_conf_clean = df_conf_clean[~df_conf_clean.author.str.contains('moderator')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title         0\n",
       "selftext     15\n",
       "subreddit     0\n",
       "author        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relationships_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "selftext     493\n",
       "subreddit      0\n",
       "author         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conf_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also observe empty selftext in both subreddits. we shall drop rows with empty selftext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean = df_relationships_clean.dropna(axis=0)\n",
    "df_conf_clean = df_conf_clean.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure only posts with selftext more than 10words are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean ['selftext_len'] = df_relationships_clean .selftext.map(lambda x: len(x.split()))\n",
    "df_relationships_clean  = df_relationships_clean [df_relationships_clean .selftext_len > 10]\n",
    "df_conf_clean['selftext_len'] = df_conf_clean.selftext.map(lambda x: len(x.split()))\n",
    "df_conf_clean = df_conf_clean[df_conf_clean.selftext_len > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships_clean.drop_duplicates(inplace=True)\n",
    "df_conf_clean.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           1801\n",
       "selftext        1801\n",
       "subreddit       1801\n",
       "author          1801\n",
       "selftext_len    1801\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "title           2286\n",
       "selftext        2286\n",
       "subreddit       2286\n",
       "author          2286\n",
       "selftext_len    2286\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_relationships_clean.count())\n",
    "display(df_conf_clean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deleted] Counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    1801\n",
       "Name: title, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False    2286\n",
       "Name: title, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[removed] Counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    1801\n",
       "Name: title, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False    2286\n",
       "Name: title, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check posts with [deleted] or [removed]\n",
    "print(\"[deleted] Counts:\")\n",
    "display((df_relationships_clean.title == '[deleted]').value_counts())\n",
    "display((df_conf_clean.title == '[deleted]').value_counts())\n",
    "print(\"[removed] Counts:\")\n",
    "display((df_relationships_clean.title == '[removed]').value_counts())\n",
    "display((df_conf_clean.title == '[removed]').value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then randomly select 1800 of each class since quite a significant number were from a moderator-author as well as empty text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_relationships_clean = df_relationships_clean.sample(n=1800,random_state=666)\n",
    "subset_conf_clean = df_conf_clean.sample(n=1800,random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confessions      1800\n",
       "relationships    1800\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine both subsets into a DF\n",
    "df = subset_relationships_clean.append(subset_conf_clean,ignore_index=True)\n",
    "df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext_len</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I (23F) can't stand my toxic boyfriend (24M) w...</td>\n",
       "      <td>I've been dating my boyfriend for about 3.5 ye...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>faultless_to_a_fault</td>\n",
       "      <td>421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I [24m] knocked up my girlfriend [22f] of nine...</td>\n",
       "      <td>As the title says, I've been with my girlfrien...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>substantial_program</td>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My BFF (17/F) keeps lying and breaks my (15/F)...</td>\n",
       "      <td>This is a teens argument so i apologise\\n in a...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>stalinesexslave2</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I (M,40) absolutely adore my partner (F,39) bu...</td>\n",
       "      <td>Iâm a m of 40 and my partner is 39. We have be...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>iam_mrgee</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My Boyfriend (25m) Called me (28f) Fat Ass.</td>\n",
       "      <td>I dont want to make this a long post so I'm go...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>smellslikecheerios</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  I (23F) can't stand my toxic boyfriend (24M) w...   \n",
       "1  I [24m] knocked up my girlfriend [22f] of nine...   \n",
       "2  My BFF (17/F) keeps lying and breaks my (15/F)...   \n",
       "3  I (M,40) absolutely adore my partner (F,39) bu...   \n",
       "4        My Boyfriend (25m) Called me (28f) Fat Ass.   \n",
       "\n",
       "                                            selftext      subreddit  \\\n",
       "0  I've been dating my boyfriend for about 3.5 ye...  relationships   \n",
       "1  As the title says, I've been with my girlfrien...  relationships   \n",
       "2  This is a teens argument so i apologise\\n in a...  relationships   \n",
       "3  Iâm a m of 40 and my partner is 39. We have be...  relationships   \n",
       "4  I dont want to make this a long post so I'm go...  relationships   \n",
       "\n",
       "                 author  selftext_len label  \n",
       "0  faultless_to_a_fault           421     0  \n",
       "1   substantial_program           305     0  \n",
       "2      stalinesexslave2           484     0  \n",
       "3             iam_mrgee           367     0  \n",
       "4    smellslikecheerios           310     0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create target class columns 0 = relationships, 1 = confessions \n",
    "\n",
    "df['label'] = df.subreddit.map({'relationships':'0','confessions':'1'}) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure formatting of text by:\n",
    "- Converting all to lower cases\n",
    "- removing groups of words in parantheses\n",
    "- remove line breaks\n",
    "- removing special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the stop words to a set.\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    #01 convert titles, selftext into lowercase\n",
    "    lower_text = text.lower()\n",
    "    #02 remove brackets and parenthesis from the title and selftext.\n",
    "    no_br_paret_text = re.sub(r'\\(.+?\\)|\\[.+?\\]',' ',str(lower_text))\n",
    "    #03 remove special characters\n",
    "    removed_special = re.sub(r'[^0-9a-zA-Z ]+',' ',str(no_br_paret_text))\n",
    "    #04 remove xamp200b\n",
    "    remove_xamp200b = re.sub(r'ampx200b',' ',str(removed_special))\n",
    "    #05 split into individual words\n",
    "    \n",
    "    return remove_xamp200b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext_len</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i   can t stand my toxic boyfriend   when we g...</td>\n",
       "      <td>i ve been dating my boyfriend for about 3 5 ye...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>faultless_to_a_fault</td>\n",
       "      <td>421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i   knocked up my girlfriend   of nine months ...</td>\n",
       "      <td>as the title says  i ve been with my girlfrien...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>substantial_program</td>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my bff   keeps lying and breaks my   heart  ho...</td>\n",
       "      <td>this is a teens argument so i apologise  in ad...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>stalinesexslave2</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i   absolutely adore my partner   but feel lik...</td>\n",
       "      <td>i m a m of 40 and my partner is 39  we have be...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>iam_mrgee</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my boyfriend   called me   fat ass</td>\n",
       "      <td>i dont want to make this a long post so i m go...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>smellslikecheerios</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  i   can t stand my toxic boyfriend   when we g...   \n",
       "1  i   knocked up my girlfriend   of nine months ...   \n",
       "2  my bff   keeps lying and breaks my   heart  ho...   \n",
       "3  i   absolutely adore my partner   but feel lik...   \n",
       "4                my boyfriend   called me   fat ass    \n",
       "\n",
       "                                            selftext      subreddit  \\\n",
       "0  i ve been dating my boyfriend for about 3 5 ye...  relationships   \n",
       "1  as the title says  i ve been with my girlfrien...  relationships   \n",
       "2  this is a teens argument so i apologise  in ad...  relationships   \n",
       "3  i m a m of 40 and my partner is 39  we have be...  relationships   \n",
       "4  i dont want to make this a long post so i m go...  relationships   \n",
       "\n",
       "                 author  selftext_len label  \n",
       "0  faultless_to_a_fault           421     0  \n",
       "1   substantial_program           305     0  \n",
       "2      stalinesexslave2           484     0  \n",
       "3             iam_mrgee           367     0  \n",
       "4    smellslikecheerios           310     0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title','selftext']] = df[['title','selftext']].applymap(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split title and self text into two classifiers where the output of title_classifier and self_text classifier would provide indication of subreddit belonging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split titles, and self text into seperate df\n",
    "\n",
    "df_title = df[['title','label']]\n",
    "df_selftext = df[['selftext','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_words(sparse_counts, columns):\n",
    "    # X_all is a sparse matrix, so sum() returns a 'matrix' datatype ...\n",
    "    #   which we then convert into a 1-D ndarray for sorting\n",
    "    word_counts = np.asarray(sparse_counts.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    # argsort() returns smallest first, so we reverse the result\n",
    "    largest_count_indices = word_counts.argsort()[::-1]\n",
    "\n",
    "    # pretty-print the results! Remember to always ask whether they make sense ...\n",
    "    freq_words = pd.Series(word_counts[largest_count_indices], \n",
    "                           index=columns[largest_count_indices])\n",
    "\n",
    "    return freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the CountVectorizer to count words for us for each class\n",
    "\n",
    "# create mask\n",
    "\n",
    "X_1 = df_selftext[df_selftext['label'] == '1']\n",
    "X_0 = df_selftext[df_selftext['label'] == '0']\n",
    "\n",
    "cvt      =  CountVectorizer(ngram_range=(1,1),stop_words='english')\n",
    "X_1_all    =  cvt.fit_transform(X_1['selftext'])\n",
    "X_0_all    =  cvt.fit_transform(X_0['selftext'])\n",
    "columns_1  =  np.array(cvt.get_feature_names())          # ndarray (for indexing below)\n",
    "columns_0  =  np.array(cvt.get_feature_names())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confessions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "invested        2206\n",
       "lagging         1870\n",
       "distant         1387\n",
       "january         1272\n",
       "unbiased        1108\n",
       "fake            1035\n",
       "tabletop        1025\n",
       "unproductive     951\n",
       "psychedelics     903\n",
       "overwhelm        840\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationships:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "just            4493\n",
       "like            4112\n",
       "don             3299\n",
       "time            3048\n",
       "know            2949\n",
       "want            2740\n",
       "feel            2528\n",
       "really          2485\n",
       "ve              2419\n",
       "relationship    2314\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_words_1 = get_freq_words(X_1_all, columns_1)\n",
    "freq_words_0 = get_freq_words(X_0_all, columns_0)\n",
    "print('Confessions:')\n",
    "display(freq_words_1[:10])\n",
    "print('Relationships:')\n",
    "display(freq_words_0[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split selftext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df_selftext['selftext']\n",
    "y_text = df_selftext['label']\n",
    "\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text,y_text,stratify=y_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Naive Bayers model - MultinomialNB as there are multiple nominal features in the form of the various tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   43.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.73      0.97      0.83       450\n",
      "     class 1       0.95      0.64      0.77       450\n",
      "\n",
      "    accuracy                           0.81       900\n",
      "   macro avg       0.84      0.81      0.80       900\n",
      "weighted avg       0.84      0.81      0.80       900\n",
      "\n",
      "Cross val score for mnb classifier using cvec vectorizer is [0.82592593 0.77962963 0.8        0.8        0.82222222]\n",
      "Accuracy score for mnb classifier using cvec vectorizer is 0.8055555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   58.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.60      0.99      0.75       450\n",
      "     class 1       0.97      0.35      0.51       450\n",
      "\n",
      "    accuracy                           0.67       900\n",
      "   macro avg       0.79      0.67      0.63       900\n",
      "weighted avg       0.79      0.67      0.63       900\n",
      "\n",
      "Cross val score for mnb classifier using tfvec vectorizer is [0.68518519 0.67407407 0.68148148 0.65740741 0.69259259]\n",
      "Accuracy score for mnb classifier using tfvec vectorizer is 0.6677777777777778\n"
     ]
    }
   ],
   "source": [
    "classifiers = [('mnb', MultinomialNB())]\n",
    "vectorizers = [('cvec', CountVectorizer(stop_words='english',tokenizer=LemmaTokenizer())),\n",
    "              ('tfvec', TfidfVectorizer(stop_words='english',tokenizer=LemmaTokenizer()))]\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    bayes_pipe = Pipeline([\n",
    "            (vectorizer),\n",
    "            (classifiers[0])\n",
    "        ])\n",
    "    scores = cross_val_score(bayes_pipe, X_text_train, y_text_train,cv=5,verbose=1)\n",
    "    b = bayes_pipe.fit(X_text_train, y_text_train)\n",
    "    y_pred = bayes_pipe.predict(X_text_test)\n",
    "    print(classification_report(y_text_test, y_pred, target_names=['class 0','class 1']))\n",
    "    print('Cross val score for {} classifier using {} vectorizer is {}'.format(classifiers[0][0],vectorizer[0],scores))\n",
    "    print('Accuracy score for {} classifier using {} vectorizer is {}'.format(classifiers[0][0],vectorizer[0],bayes_pipe.score(X_text_test, y_text_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words='english',tokenizer=LemmaTokenizer())),\n",
    "    ('lr', LogisticRegression(solver='saga',max_iter=300))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform GridSearchCV to get best hyperparameters for our vectorizer and logistic regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 270 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed: 27.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822962962962963\n"
     ]
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2500, 3000, 3500],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__penalty' : ['elasticnet'],\n",
    "    'lr__C' : np.arange(0.1,1,0.1),\n",
    "    'lr__l1_ratio' : np.arange(0.1,1.1,0.2)\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3,verbose=1,n_jobs=-1)\n",
    "gs.fit(X_text_train, y_text_train)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 3000,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'lr__C': 0.30000000000000004,\n",
       " 'lr__l1_ratio': 0.1,\n",
       " 'lr__penalty': 'elasticnet'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_.score(X_text_test,y_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try model on title\n",
    "optimal_pipe = Pipeline([\n",
    "            ('cvec', CountVectorizer(tokenizer=LemmaTokenizer(),max_features=3000,ngram_range=(1,1))),\n",
    "            ('lr', LogisticRegression(solver='saga',max_iter=300,C=0.3,l1_ratio=0.1,penalty='elasticnet'))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_title = df_title['title']\n",
    "y_title = df_title['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cvec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=3000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<__main__.LemmaTokenizer object at 0x1085d40f0>,\n",
       "                                 vocabulary=None)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=0.3, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=0.1, max_iter=300,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='elasticnet', random_state=None,\n",
       "                                    solver='saga', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_pipe.fit(X_text_train, y_text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try the model on our `title` dataset to obtain the accuracy of the model to classify the subreddit from titles alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.81      0.93      0.86       450\n",
      "     class 1       0.91      0.78      0.84       450\n",
      "\n",
      "    accuracy                           0.85       900\n",
      "   macro avg       0.86      0.85      0.85       900\n",
      "weighted avg       0.86      0.85      0.85       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_logr_pred = optimal_pipe.predict(X_text_test)\n",
    "print(classification_report(y_text_test, y_logr_pred, target_names=['class 0','class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we explore the use tfidfvectorizer instead of countvectorizer to account for document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 270 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 38.8min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed: 39.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8585185185185186\n"
     ]
    }
   ],
   "source": [
    "tfidf_pipe = Pipeline([\n",
    "    ('tfvec', TfidfVectorizer(stop_words='english',tokenizer=LemmaTokenizer())),\n",
    "    ('lr', LogisticRegression(solver='saga',max_iter=300))\n",
    "])\n",
    "\n",
    "tfidf_params = {\n",
    "    'tfvec__max_features': [2500, 3000, 3500],\n",
    "    'tfvec__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__penalty' : ['elasticnet'],\n",
    "    'lr__C' : np.arange(0.1,1,0.1),\n",
    "    'lr__l1_ratio' : np.arange(0.1,1.1,0.2)\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(tfidf_pipe, param_grid=tfidf_params, cv=3,verbose=1,n_jobs=-1)\n",
    "gs.fit(X_text_train, y_text_train)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 0.8,\n",
       " 'lr__l1_ratio': 0.1,\n",
       " 'lr__penalty': 'elasticnet',\n",
       " 'tfvec__max_features': 3500,\n",
       " 'tfvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Report (results based on test data) \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.86      0.90      0.88       450\n",
      "     class 1       0.89      0.86      0.87       450\n",
      "\n",
      "    accuracy                           0.88       900\n",
      "   macro avg       0.88      0.88      0.88       900\n",
      "weighted avg       0.88      0.88      0.88       900\n",
      "\n",
      "Titles (all titles) Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.21      0.34      1800\n",
      "     class 1       0.55      0.98      0.71      1800\n",
      "\n",
      "    accuracy                           0.59      3600\n",
      "   macro avg       0.74      0.59      0.52      3600\n",
      "weighted avg       0.74      0.59      0.52      3600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test model against test text data and rest of titles\n",
    "y_text_tfidf_pred = gs.best_estimator_.predict(X_text_test)\n",
    "y_title_tfidf_pred = gs.best_estimator_.predict(X_title)\n",
    "print(\"Text Report (results based on test data) \\n\" + \n",
    "      classification_report(y_text_test, y_text_tfidf_pred, target_names=['class 0','class 1']))\n",
    "print(\"Titles (all titles) Report \\n\" + \n",
    "      classification_report(y_title, y_title_tfidf_pred, target_names=['class 0','class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline([\n",
    "    ('tfvec', TfidfVectorizer(stop_words='english',tokenizer=LemmaTokenizer())),\n",
    "    ('lr', LogisticRegression(solver='saga',max_iter=300))\n",
    "])\n",
    "\n",
    "tfidf_params = {\n",
    "    'tfvec__max_features': [2500, 3000, 3500],\n",
    "    'tfvec__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__penalty' : ['elasticnet'],\n",
    "    'lr__C' : np.arange(0.1,1,0.1),\n",
    "    'lr__l1_ratio' : np.arange(0.1,1.1,0.2)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
